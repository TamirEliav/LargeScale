{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_insight_artificial_data_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyx-JPngFw35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r!rm -rf raw_data\n",
        "!rm -rf position_decoder.h5\n",
        "!rm -rf logs\n",
        "!rm -rf models\n",
        "# !rm -rf \"../models\"\n",
        "# !cp -r \"../models\" \"./\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwv2EnXqtiP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -e git+https://github.com/CYHSM/DeepInsight.git#egg=DeepInsight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh0grcw2cTmw",
        "colab_type": "code",
        "outputId": "4ef45cd6-a239-4946-f897-015487e21a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "# update tensorflow For a specific version:\n",
        "!pip install tensorflow==1.15.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 106kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.28.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.16.4)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.9.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=0ed9deb3fe4ad71dcc3eda35d22e22a721654b18e44bf5648726d44c42370688\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: deepinsight 0.5 has requirement tensorflow==1.12.2, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arKNhI71V4_Y",
        "colab_type": "code",
        "outputId": "b3cab7dd-4d08-4668-e5fe-b649150e5aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__) \n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fbe1d02f927d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o6nbXy2tnDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import DeepInsight\n",
        "import deepinsight\n",
        "# Choose GPU\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTeMOEBQwcuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import h5py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX5XoTQ83xnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create output (position)\n",
        "T=40\n",
        "fs_output = 30\n",
        "dt=1/fs_output\n",
        "f=0.15;\n",
        "t=np.arange(0,T,dt)\n",
        "# position=100*(1-np.cos(2*np.pi*f*t))\n",
        "position=(sp.signal.sawtooth(2*np.pi*f*t)+1)*100\n",
        "plt.plot(t,position,'.-')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('position (m/s)')\n",
        "output = position\n",
        "output_timestemps = t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_6zUvlas9M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create input (spikes)\n",
        "nCells=16\n",
        "nPosBins = 1000;\n",
        "field_width=200/nCells\n",
        "maxFR = 40;\n",
        "xpos=np.linspace(0,200,nPosBins)\n",
        "cells_field_locs=np.linspace(0,200,nCells)\n",
        "cells_spike_size=np.linspace(100,1000,nCells)\n",
        "fs_raw=30000\n",
        "dt2=1/fs_raw\n",
        "t2=np.arange(0,T,dt2)\n",
        "maps=np.zeros((nCells,len(xpos)))\n",
        "# raw_data=np.zeros((nCells,len(t2)))\n",
        "raw_data=np.zeros((1,len(t2)))\n",
        "spike_wvf=np.array( [0,10,20,40,80,160,320,400,350,230,180,130,100,70,30,0,-30,-60,-90,-60,-30,0] )\n",
        "spike_wvf=spike_wvf / max(spike_wvf)\n",
        "position2=np.interp(t2,t,position)\n",
        "# noise=np.random.normal(0,10,len(position2))\n",
        "# noise[abs(noise<1)]=3\n",
        "# position2=position2+noise\n",
        "fp_hdf_raw_data = 'raw_data'\n",
        "hdf5_file = h5py.File(fp_hdf_raw_data, mode='a')\n",
        "hdf5_file.create_dataset(\"data\", [len(t2),nCells], np.float16)\n",
        "np.random.seed(0)\n",
        "for ii_cell,mu in enumerate(cells_field_locs):\n",
        "  # first, create FR map\n",
        "  print(ii_cell,mu)\n",
        "  FR_map=sp.stats.norm(mu, field_width).pdf(xpos)\n",
        "  FR_map=FR_map/max(FR_map)*maxFR\n",
        "  maps[ii_cell,:]=FR_map\n",
        "  # now, apply the FR with the position variable\n",
        "  FR=np.interp(position2,xpos,FR_map);\n",
        "  # FRs[ii_cell,:]=FR\n",
        "  spikes=np.random.poisson(FR*dt2)\n",
        "  spikes=np.convolve(spikes,spike_wvf*cells_spike_size[ii_cell],mode='same')\n",
        "  # raw_data[ii_cell,:]=spikes\n",
        "  raw_data=raw_data+spikes\n",
        "  # write data to hdf5 file\n",
        "  hdf5_file[\"data\"][:,ii_cell] = spikes\n",
        "  hdf5_file.flush()\n",
        "hdf5_file.close()\n",
        "np.shape(raw_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGM-uBn4q00T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hdf5_file = h5py.File(fp_hdf_raw_data, mode='a')\n",
        "if not dataset_name in hdf5_file:\n",
        "  hdf5_file.create_dataset(dataset_name, dataset_shape, dataset_type)  \n",
        "hdf5_file.create_dataset(\"data2\", [len(t2),nCells/2], np.float16)\n",
        "hdf5_file[\"data2\"] = np.reshape(np.array(hdf5_file[\"data\"]),(len(t2),nCells/2,2))\n",
        "hdf5_file.flush()\n",
        "hdf5_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGl6RJ4OrxUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(maps.transpose())\n",
        "plt.xlabel('position')\n",
        "plt.ylabel('FR (Hz)')\n",
        "plt.title('FR maps')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FX-r8ZiuDcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.plot(raw_data[::2,0*fs_raw:20*fs_raw].transpose())\n",
        "plt.plot(raw_data.sum(axis=0)[0*fs_raw:20*fs_raw],color='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSaiBqgy1M_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge all channels to one single channel! (multiplexing...)\n",
        "inputs=raw_data.sum(axis=0)[:, np.newaxis]\n",
        "plt.plot(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Avkztx2kBR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy.matlib\n",
        "inputs=np.matlib.repmat(raw_data,2,1).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1aOp2yFsi-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fp_deepinsight = '/content/position_decoder.h5'\n",
        "fp_deepinsight = './position_decoder.h5'\n",
        "ds=1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxw_PcVuqDPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess input - Transform raw data to frequency domain\n",
        "deepinsight.preprocess.preprocess_input(fp_deepinsight, inputs, sampling_rate=fs_raw, average_window=ds, num_cores=20)\n",
        "\n",
        "# hdf5_file = h5py.File(fp_hdf_raw_data, mode='r')\n",
        "# deepinsight.preprocess.preprocess_input(fp_deepinsight, hdf5_file['data'], sampling_rate=fs_raw, average_window=ds, num_cores=20)\n",
        "# hdf5_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSKCZdFUL6dZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_create_or_update(hdf5_file, dataset_name, dataset_shape, dataset_type, dataset_value):\n",
        "    \"\"\"\n",
        "    Create or update dataset in HDF5 file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hdf5_file : File\n",
        "        File identifier\n",
        "    dataset_name : str\n",
        "        Name of new dataset\n",
        "    dataset_shape : array_like\n",
        "        Shape of new dataset\n",
        "    dataset_type : type\n",
        "        Type of dataset (np.float16, np.float32, 'S', etc...)\n",
        "    dataset_value : array_like\n",
        "        Data to store in HDF5 file\n",
        "    \"\"\"\n",
        "    if not dataset_name in hdf5_file:\n",
        "        hdf5_file.create_dataset(dataset_name, dataset_shape, dataset_type)\n",
        "        hdf5_file[dataset_name][:] = dataset_value\n",
        "    else:\n",
        "        hdf5_file[dataset_name][:] = dataset_value\n",
        "    hdf5_file.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrVWBzTr0sQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess outputs - align output timestemps\n",
        "hdf5_file = h5py.File(fp_deepinsight, mode='a')\n",
        "# Get size of wavelets\n",
        "input_length = hdf5_file['inputs/wavelets'].shape[0]\n",
        "# downsample output\n",
        "position3 = np.interp(t2[np.arange(0,t2.shape[0],ds)], t, position)\n",
        "# noise=np.random.normal(0,10,len(position3))\n",
        "# noise[abs(noise<1)]=3\n",
        "# position3=position3+noise\n",
        "# Create and save datasets in HDF5 File\n",
        "my_create_or_update(hdf5_file, dataset_name=\"outputs/position\", \n",
        "                      dataset_shape=[input_length, 1], dataset_type=np.float16, dataset_value=position3[0: input_length, np.newaxis])\n",
        "\n",
        "hdf5_file.flush()\n",
        "hdf5_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfOC5Z_XQzw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # check inputs and outputs!\n",
        "hdf5_file = h5py.File(fp_deepinsight, mode='r')\n",
        "wvlts = hdf5_file['inputs/wavelets']\n",
        "ff = hdf5_file['inputs/fourier_frequencies']\n",
        "position_out = hdf5_file['outputs/position']\n",
        "print(wvlts)\n",
        "print(ff)\n",
        "print(position_out)\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(position_out[:],'.',color=\"blue\")\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(wvlts[:,:,1])\n",
        "plt.subplot(3,1,3)\n",
        "plt.imshow(wvlts[:,:,0].transpose(),aspect='auto')\n",
        "# plt.colorbar()\n",
        "# plt.plot(np.mean(wvlts,axis=1)[:700,:])\n",
        "# print(np.shape(np.mean(wvlts,axis=1)))\n",
        "\n",
        "# plt.plot(ff,'.-')\n",
        "print(ff[7])\n",
        "plt.show()\n",
        "hdf5_file.close()\n",
        "np.shape(t2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIwt4p2AY3rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r-XXJkpYJwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def myRun(path_in, loss_functions, loss_weights, num_cvs=5):\n",
        "    \"\"\"\n",
        "    Runs model training giving path to HDF5 file and loss dictionaries\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_in : str\n",
        "        Path to HDF5\n",
        "    loss_functions : dict\n",
        "        For each output the corresponding loss function\n",
        "    loss_weights : dict\n",
        "        For each output the corresponding weight\n",
        "    num_cvs : int, optional\n",
        "        Number of cross validation splits, by default 5\n",
        "    \"\"\"\n",
        "    dirname = os.path.dirname(path_in)\n",
        "    filename = os.path.basename(path_in)\n",
        "    # Define folders\n",
        "    tensorboard_logfolder = dirname + '/logs/' + filename[0:-3]  # Remove .h5 for logfolder\n",
        "    model_tmp_path = dirname + '/models/tmp/tmp_model'\n",
        "    model_path = dirname + '/models/' + filename[0:-3] + '_model.h5'\n",
        "    # Create folders if needed\n",
        "    for f in [os.path.dirname(model_tmp_path), os.path.dirname(model_path)]:\n",
        "        if not os.path.exists(f):\n",
        "            os.makedirs(f)\n",
        "    print('------------------------------------------------')\n",
        "    print('-> Running {} from {}'.format(filename, dirname))\n",
        "    print('- Logs : {} \\n- Model temporary : {} \\n- Model : {}'.format(tensorboard_logfolder, model_tmp_path, model_path))\n",
        "    print('------------------------------------------------')\n",
        "    # Train model\n",
        "    print('------------------------------------------------')\n",
        "    print('Starting standard model')\n",
        "    print('------------------------------------------------')\n",
        "    deepinsight.train.train_model(model_path, path_in, tensorboard_logfolder, model_tmp_path, loss_functions, loss_weights, num_cvs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMAQYbD7xTnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN!!!\n",
        "\n",
        "# Define loss functions and train model\n",
        "loss_functions = {'position' : 'euclidean_loss'}\n",
        "loss_weights = {'position' : 1}\n",
        "\n",
        "# deepinsight.train.run_from_path(fp_deepinsight, loss_functions, loss_weights)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  myRun(fp_deepinsight, loss_functions, loss_weights, num_cvs=2)\n",
        "  # deepinsight.train.run_from_path(fp_deepinsight, loss_functions, loss_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOkvX804ePj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses, output_predictions, indices = deepinsight.analyse.get_model_loss(fp_deepinsight,stepsize=1)\n",
        "shuffled_losses = deepinsight.analyse.get_shuffled_model_loss(fp_deepinsight, axis=1,stepsize=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgva53Ptt82d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplot(2,1,1)\n",
        "plt.plot(position3)\n",
        "plt.plot(indices,output_predictions['position'])\n",
        "plt.legend(['original','predicted'])\n",
        "plt.subplot(2,1,2)\n",
        "xxx1=0\n",
        "xxx2=100\n",
        "plt.plot(indices[xxx1:xxx2],position3[xxx1:xxx2])\n",
        "plt.plot(indices[xxx1:xxx2],output_predictions['position'][xxx1:xxx2])\n",
        "plt.legend(['original','predicted'])\n",
        "plt.savefig('position_decoding.pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVlHeqhADCwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K171dYpjRE_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_predictions['position'][:64].transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQaM7NMNY-8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(position3, output_predictions['position'], '.')\n",
        "# plt.plot(position3[xxx1:xxx2], output_predictions['position'][xxx1:xxx2], '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Z09EorajtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_predictions['position'][:66].transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5vmn3lkZwoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(shuffled_losses[:,:,0].transpose())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrPoRs0wb3uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.diff(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}